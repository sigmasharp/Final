{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade numpy\n",
    "#!pip install --upgrade nltk\n",
    "#!pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnsm' from 'rnnsm.pyc'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnsm; reload(rnnsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-c8e7c32631d4>:17 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-38-c8e7c32631d4>:17 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import rnnsm; reload(rnnsm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  sm = rnnsm.RNNSM(V=10000, Z=6, H=200, num_layers=2)\n",
    "  sm.BuildCoreGraph()\n",
    "  sm.BuildTrainGraph()\n",
    "  sm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(sm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = sm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = sm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = sm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "        h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    " \n",
    "    cost, h, _ = session.run([loss, sm.final_h_, train_op], feed_dict= {sm.target_y_: y, sm.initial_h_:h,\n",
    "        sm.input_w_: w, sm.dropout_keep_prob_:keep_prob, sm.learning_rate_:learning_rate})      \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat, 17 Dec 2016 23:43:10\n",
      "Loaded 130051 sentences (4.28041e+06 tokens)\n",
      "Loaded 130051 sentiments (130051 tokens)\n",
      "Training set: 65025 sentences (2140632 tokens)\n",
      "Test set: 32513 sentences (1069635 tokens)\n",
      "dev set: 32513 sentences (1070139 tokens)\n",
      "Training set: 65025 sentiments (65025 tokens)\n",
      "Test set: 32513 sentiments (32513 tokens)\n",
      "dev set: 32513 sentiments (32513 tokens)\n",
      "Sat, 17 Dec 2016 23:43:51\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import time\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "reload(utils)\n",
    "V = 10000\n",
    "Z = 4\n",
    "vocab, svocab, train_ids, train_sids, test_ids, test_sids, dev_ids, dev_sids, test_sents, test_sentis = \\\n",
    "    utils.load_data(\"text.full.txt\", \"sn0p.new.full.txt\", train=0.5, test=0.25, V=V, Z=Z, shuffle=True)\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  498   46  473   11   24   48  535   42   35    4  124   10 1086   25\n",
      "   16 1325    9   10  737 1545  204 1739  530    5    0 2028    8  803   11\n",
      " 1998  307   44   50   59 2110  209    7   97   11  458 2660  236    5    0\n",
      "  585   10 1098    7   10 4836    2    3    4    2 5636   25    8 1910  207\n",
      "  173   19  810    6 1834   67    2    2    5    0   74 1494 1390    5    0\n",
      " 3570   11 1785   99    4 2948   16  590    6  509   10 9944  806 2008    8\n",
      "   39 2882  196  980    3   30 1620 6855 9282    2]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[   0    6    2    4 9485    3  216  145  509   10  181  204   33 2393    8\n",
      " 3039 3025   42   35 8513    3  551    6    4  181  204   33 2398 5939 2229\n",
      "    3  255    4 2398 5170    5    0    9   75    3   33   38    2 2680    6\n",
      "   10    2    3   30   54  313   12  100   31   57    6  572   12   10  463\n",
      "    9 4594  667  326  830   13   53  320    5    0  555  111   47   41 8341\n",
      "   19  189  187   29  115   24    9    4  106  105    3   66   18   12  202\n",
      " 1621    9   10  267  289   31 7740  110   20   31]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[   0   20   18  705   12   22   42  271   10  479  929  368    9    2    3\n",
      "   10  680  615    2  248 2960   26  193  237  201   60    5    0  102  354\n",
      "   23  443  759  123   16 1079   26  123  136    0  110   38 2848   11  168\n",
      "   31  903 1713   26   20    3    4  159 3509   12   14 1608    9  164   26\n",
      "   20   16 1636   39  370   26    4 1087    6  805    6    2    4 4005    8\n",
      "  521    4 1801    3   30    9    4 2402    7  583 2450    3   10 1080  403\n",
      " 1607   13    4  322  241    5    0  223 2626    4]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[ [u'To', u'sidestep', u'the', u'wrangling', u',', u'Interpublic', u'may', u'create', u'a', u'unit', u'under', u'which', u'Draft', u'and', u'Foote', u'Cone', u'would', u'be', u'housed', u',', u'similar', u'to', u'the', u'unit', u'under', u'which', u'McCann', u'Erickson', u'operates', u',', u'called', u'the', u'McCann', u'Worldgroup', u'.']\n",
      " [u'In', u'one', u',', u'which', u'Mr.', u'Pickrell', u'attributed', u'to', u'a', u\"''reader\", u',', u\"''\", u'he', u'reported', u'that', u'Wal-Mart', u'was', u'about', u'to', u'announce', u'that', u'a', u'store', u'in', u'Illinois', u'received', u'25,000', u'applications', u'for', u'325', u'jobs', u'.']\n",
      " [u'Although', u'analysts', u'had', u'not', u'counted', u'on', u'much', u'revenue', u'from', u'Windows', u'98', u'in', u'the', u'first', u'quarter', u',', u'they', u'said', u'that', u'any', u'delay', u'in', u'a', u'major', u'product', u'was', u'worrisome', u'because', u'Microsoft', u'was', u'entering', u'a', u'period', u'of', u'slower', u'growth', u'.']\n",
      " [u'Chinese', u'authorities', u'are', u'asking', u'suppliers', u'of', u'commercial', u'aircraft', u'to', u'delay', u'deliveries', u'to', u'nation', u\"'s\", u'airlines', u';', u'Chinese', u'airlines', u'suffer', u'from', u'serious', u'overcapacity', u';', u'fare', u'wars', u'and', u'half-empty', u'flights', u',', u'unknown', u'in', u'China', u'until', u'recently', u',', u'are', u'problems', u'for', u'China', u\"'s\", u'two', u'dozen', u'airline', u'companies', u';', u'delays', u'are', u'bad', u'news', u'for', u'Boeing', u'Co', u'and', u'Airbus', u'Industrie', u',', u'which', u'eagerly', u'compete', u'for', u'business', u'in', u'China', u';', u'executives', u'at', u'each', u'company', u'seem', u'surprised', u',', u'even', u'skeptical', u',', u'of', u'China', u\"'s\", u'announcement', u'(', u'M', u')']\n",
      " [u'Two', u'were', u'drug', u'stocks', u',', u'Merck', u'and', u'Pfizer', u',', u'whose', u'painkillers', u'were', u'linked', u'to', u'heart', u'problems', u'.']\n",
      " [u'Between', u'1986', u'and', u'1992', u',', u'Mr.', u'Coughlin', u'was', u'responsible', u'for', u'investigating', u'employee', u'theft', u'and', u'abuse', u'at', u'Wal-Mart', u\"'s\", u'Sam', u\"'s\", u'Club', u'warehouse', u'unit', u',', u'the', u'complaint', u'said', u'.']\n",
      " [u'Citigroup', u'also', u'announced', u'a', u'14', u'percent', u'dividend', u'increase', u',', u'to', u'40', u'cents', u'a', u'share', u'.']\n",
      " [u'Internet', u'Explorer', u'is', u'tightly', u'bound', u'to', u'Windows', u',', u'a', u'move', u'that', u'Microsoft', u'says', u'improves', u'the', u'browser', u\"'s\", u'performance', u'.']\n",
      " [u\"''I\", u'hate', u'not', u'being', u'able', u'to', u'call', u'you', u'or', u'write', u'you', u',', u\"''\", u'Ms.', u'Roehm', u'wrote', u'early', u'last', u'fall', u',', u'according', u'to', u'an', u'e-mail', u'message', u'Mr.', u'Womack', u\"'s\", u'wife', u'provided', u'to', u'Wal-Mart', u'.']\n",
      " [u\"''This\", u'is', u'a', u'significant', u'portion', u'of', u'the', u'vigorous', u'legal', u'defense', u'that', u'we', u'promised', u'to', u'deliver', u'on', u'May', u'14', u'when', u'we', u'responded', u'to', u'the', u'Digital', u'Equipment', u'lawsuit', u',', u\"''\", u'said', u'Chuck', u'Mulloy', u',', u'an', u'Intel', u'spokesman', u'.']\n",
      " [u'Microsoft', u'is', u'in', u'a', u'bitter', u'battle', u'with', u'the', u'Netscape', u'Communications', u'Corporation', u'and', u'both', u'companies', u'have', u'been', u'rushing', u'out', u'new', u'versions', u'of', u'their', u'Internet', u'browser', u'programs', u'in', u'an', u'effort', u'to', u'offer', u'new', u'features', u'.']\n",
      " [u\"''The\", u'plaintiffs', u'are', u'failing', u'in', u'the', u'effort', u'to', u'extort', u'money', u'from', u'Merck', u',', u\"''\", u'said', u'Michael', u'Krensavage', u',', u'a', u'drug', u'industry', u'analyst', u'at', u'Raymond', u'James', u'.']\n",
      " [u\"''It\", u\"'s\", u'something', u'they', u'should', u'have', u'done', u'two', u'or', u'three', u'years', u'ago', u',', u\"''\", u'said', u'Morris', u'Mark', u'of', u'Mark', u'Asset', u'Management', u',', u'which', u'owned', u'nearly', u'612,000', u'Time', u'Warner', u'shares', u'at', u'the', u'end', u'of', u'March', u'.']\n",
      " [u'The', u'Bank', u'of', u'America', u'unit', u'of', u'the', u'BankAmerica', u'Corporation', u'has', u'named', u'four', u'finalists', u'to', u'compete', u'for', u'its', u'combined', u'corporate', u'image', u'and', u'retail', u'accounts', u',', u'with', u'billings', u'estimated', u'at', u'$', u'50', u'million', u'to', u'$', u'70', u'million', u'.']\n",
      " [u'The', u'International', u'Business', u'Machines', u'Corporation', u'plans', u'to', u'announce', u'a', u'new', u'semiconductor', u'design', u'today', u'that', u'it', u'asserts', u'could', u'greatly', u'improve', u'the', u'performance', u'or', u'reduce', u'the', u'power', u'consumption', u'of', u'wireless', u'devices', u'like', u'cellphones', u'compared', u'with', u'today', u\"'s\", u'technology', u'.']\n",
      " [u'But', u'it', u'is', u'not', u'so', u'clear', u'that', u'the', u'AOL-Time', u'Warner', u'distribution', u'networks', u'would', u'always', u'give', u'it', u'the', u'upper', u'hand', u'.']\n",
      " [u'In', u'a', u'related', u'action', u'today', u',', u'Sun', u'Microsystems', u',', u'a', u'bitter', u'competitor', u'to', u'Microsoft', u',', u'asked', u'a', u'Federal', u'court', u'to', u'block', u'the', u'release', u'of', u'Windows', u'98', u'until', u'Microsoft', u'alters', u'the', u'program', u'to', u'include', u'a', u'Sun-approved', u'version', u'of', u'Sun', u\"'s\", u'Java', u'programming', u'language', u'.']\n",
      " [u'But', u'unlike', u'Intel', u',', u'whose', u'plants', u'are', u'kept', u'busy', u'churning', u'out', u'hundreds', u'of', u'millions', u'of', u'microprocessors', u'that', u'run', u'personal', u'computers', u',', u'I.B.M', u'.']\n",
      " [u'The', u'T.', u'Rowe', u'Price', u'fund', u'does', u'have', u'a', u'big', u'position', u'in', u'General', u'Electric', u',', u'mainly', u'because', u'G.E', u'.']\n",
      " [u'The', u'court', u'paper', u'states', u'that', u'Mr.', u'Jones', u\"''admitted\", u'that', u'Microsoft', u\"'absolutely\", u\"'\", u'intended', u'to', u'persuade', u'Netscape', u'not', u'to', u'compete', u'.', u\"''\"]\n",
      " [u'Mr.', u'Spitzer', u\"'s\", u'office', u'negotiated', u'with', u'I.B.M', u'.']\n",
      " [u\"'s\", u'well', u'over', u'130', u',', u\"''\", u'said', u'Douglas', u'Coupland', u',', u'whose', u'novel', u\"''Microserfs\", u\"''\", u'satirized', u'Microsoft', u\"'s\", u'geeky', u'culture', u'.']\n",
      " [u'I.B.M', u'.']\n",
      " [u'As', u'William', u'H.', u'Gates', u'opened', u'his', u'annual', u'speech', u'at', u'the', u'Comdex', u'technology', u'conference', u',', u'he', u'tried', u'to', u'portray', u'Microsoft', u'as', u'a', u'company', u'that', u'wanted', u'to', u'give', u'consumers', u'choices', u'in', u'how', u'they', u'use', u'computers', u'and', u'get', u'on', u'line', u'.']\n",
      " [u'Mr.', u'Jobs', u'also', u'spent', u'considerable', u'time', u'demonstrating', u'the', u'new', u'version', u'of', u'Apple', u\"'s\", u'operating', u'system', u',', u'Mac', u'OS', u'version', u'8.5', u'.']\n",
      " [u'Apple', u'Computer', u'introduces', u'iPod', u'Hi-Fi', u',', u'all-in-one', u'speaker', u'system', u'housed', u'in', u'case', u'about', u'size', u'of', u'shoebox', u'and', u'priced', u'at', u'$', u'349', u',', u'for', u'iPod', u'music', u'player', u';', u'also', u'introduces', u'new', u'version', u'of', u'MacMini', u'computer', u'with', u'features', u'for', u'managing', u'digital', u'music', u'and', u'video', u';', u'iPod', u'docking', u'device', u',', u'though', u'not', u'being', u'promoted', u'as', u'portable', u',', u'can', u'run', u'off', u'six', u'D', u'batteries', u'or', u'AC', u'power', u',', u'and', u'comes', u'with', u'remote', u'control', u';', u'photo', u'(', u'M', u')']\n",
      " [u\"''Google\", u'is', u'going', u'into', u'new', u'channels', u'like', u'video', u'and', u'Yahoo', u'is', u'still', u'trying', u'to', u'fix', u'their', u'core', u'channel', u'.', u\"''\"]\n",
      " [u'But', u'negotiations', u'are', u'sensitive', u'for', u'another', u'reason', u':', u'I.B.M.', u',', u'which', u'licenses', u'the', u'Macintosh', u'operating', u'system', u'from', u'Apple', u'to', u'supply', u'to', u'several', u'other', u'clone', u'makers', u',', u'is', u'only', u'one', u'of', u'two', u'suppliers', u'of', u'the', u'Power', u'PC', u'chip', u'that', u'serves', u'as', u'the', u'engine', u'for', u'all', u'Macintoshes', u'sold', u'today', u'.']\n",
      " [u'For', u'example', u',', u'the', u'News', u'Corporation', u'just', u'announced', u'a', u'$', u'3', u'billion', u'share', u'repurchase', u',', u'which', u'is', u'twice', u'as', u'large', u'as', u'its', u'2001', u'program', u',', u'and', u'Viacom', u\"'s\", u'current', u'$', u'8', u'billion', u'buyback', u'plan', u'dwarfs', u'former', u'programs', u'.']\n",
      " [u'MY', u'wife', u'and', u'I', u'are', u'victims', u'of', u'Netflix', u',', u'and', u'it', u'is', u'our', u'own', u'fault', u'.']]\n",
      "[['p']\n",
      " ['0']\n",
      " ['n']\n",
      " ['0']\n",
      " ['0']\n",
      " ['n']\n",
      " ['0']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['0']\n",
      " ['p']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['n']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']\n",
      " ['p']\n",
      " ['0']\n",
      " ['0']\n",
      " ['0']]\n",
      "2205658 2205658 1102149 1102149 1102653 1102653\n"
     ]
    }
   ],
   "source": [
    "print train_ids[:100]\n",
    "print train_sids[:100]\n",
    "print test_ids[:100]\n",
    "print test_sids[:100]\n",
    "print dev_ids[:100]\n",
    "print dev_sids[:100]\n",
    "print test_sents[:30]\n",
    "print test_sentis[:30]\n",
    "print len(train_ids), len(train_sids), len(test_ids), len(test_sids), len(dev_ids), len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100,\n",
    "                    Z=Z,\n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = './tf_saved/tf_saved_rnnsm_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(sm, session, ids, sids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, sids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(sm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat, 17 Dec 2016 23:45:16\n",
      "WARNING:tensorflow:From <ipython-input-47-26d33fab470b>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-47-26d33fab470b>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sat, 17 Dec 2016 23:49:00\n",
      "[epoch 1] Completed in 0:03:42\n",
      "[epoch 1] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.922  (perplexity: 2.51)\n",
      "[epoch 1] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.936  (perplexity: 2.55)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sat, 17 Dec 2016 23:53:44\n",
      "[epoch 2] Completed in 0:03:46\n",
      "[epoch 2] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.897  (perplexity: 2.45)\n",
      "[epoch 2] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.926  (perplexity: 2.52)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sat, 17 Dec 2016 23:58:32\n",
      "[epoch 3] Completed in 0:03:48\n",
      "[epoch 3] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.866  (perplexity: 2.38)\n",
      "[epoch 3] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.912  (perplexity: 2.49)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sun, 18 Dec 2016 00:03:31\n",
      "[epoch 4] Completed in 0:04:00\n",
      "[epoch 4] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.849  (perplexity: 2.34)\n",
      "[epoch 4] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.909  (perplexity: 2.48)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sun, 18 Dec 2016 00:08:49\n",
      "[epoch 5] Completed in 0:04:15\n",
      "[epoch 5] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.829  (perplexity: 2.29)\n",
      "[epoch 5] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.896  (perplexity: 2.45)\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sun, 18 Dec 2016 00:14:07\n",
      "[epoch 6] Completed in 0:04:16\n",
      "[epoch 6] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.809  (perplexity: 2.25)\n",
      "[epoch 6] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.899  (perplexity: 2.46)\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sun, 18 Dec 2016 00:19:31\n",
      "[epoch 7] Completed in 0:04:20\n",
      "[epoch 7] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.796  (perplexity: 2.22)\n",
      "[epoch 7] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.902  (perplexity: 2.46)\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sun, 18 Dec 2016 00:24:56\n",
      "[epoch 8] Completed in 0:04:23\n",
      "[epoch 8] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.772  (perplexity: 2.16)\n",
      "[epoch 8] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.907  (perplexity: 2.48)\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sun, 18 Dec 2016 00:30:25\n",
      "[epoch 9] Completed in 0:04:26\n",
      "[epoch 9] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.762  (perplexity: 2.14)\n",
      "[epoch 9] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.909  (perplexity: 2.48)\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "in batch_generator 2205658 2205658 2205650 2205650 2205650 50\n",
      "Sun, 18 Dec 2016 00:35:54\n",
      "[epoch 10] Completed in 0:04:27\n",
      "[epoch 10] in batch_generator 2205658 2205658 2205600 2205600 2205600 100\n",
      "Train set: avg. loss: 0.749  (perplexity: 2.12)\n",
      "[epoch 10] in batch_generator 1102653 1102653 1102600 1102600 1102600 100\n",
      "Test set: avg. loss: 0.914  (perplexity: 2.49)\n",
      "\n",
      "Sun, 18 Dec 2016 00:36:57\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "reload(utils)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  os.environ['TZ'] = 'US/Pacific'\n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    sm = rnnsm.RNNSM(**model_params)\n",
    "    sm.BuildCoreGraph()\n",
    "    sm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, train_sids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    run_epoch(sm, session, bi, train=True, keep_prob=keep_prob, learning_rate=learning_rate)\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, train_ids, train_sids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, dev_ids, dev_sids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, './tf_saved/tf_saved_rnnsm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)\n",
    "  \n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_step(sm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    sm : rnnsm.RNNSM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  final_h, samples = session.run([sm.final_h_, sm.pred_samples_], \n",
    "        feed_dict={sm.input_w_: input_w, sm.initial_h_: initial_h, sm.dropout_keep_prob_: 1.0, sm.learning_rate_:0.1})\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_predict(sm, session, seq, vocab, svocab):\n",
    "  \"\"\"Score by test_ids vs test_sids\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  h, y = sample_step(sm, session, w[:,-1:], h)\n",
    "\n",
    "  y = [1 if k == 3 else k for k in utils.flatten(y)]\n",
    "\n",
    "  #return [svocab.ids_to_words(k) for k in y]\n",
    "  return svocab.ids_to_words(y)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: 2311 out of 11547  correct, and total dev is  32513\n",
      "Accuracy rate is 0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        sm = rnnsm.RNNSM(**model_params)\n",
    "        sm.BuildCoreGraph()\n",
    "        sm.BuildSamplerGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './'+trained_filename)\n",
    "    pred = []\n",
    "\n",
    "    for s in test_sents:\n",
    "        pred.append(seq_predict(sm, session, s, vocab, svocab))\n",
    "\n",
    "    non0 = 0\n",
    "    correct = 0\n",
    "    for i in range(len(test_sents)):\n",
    "        if not test_sentis[i][0] == '0':\n",
    "            non0 = non0 + 1\n",
    "            if pred[i] == test_sentis[i][0]:\n",
    "                correct = correct + 1\n",
    "print \"Test result:\", correct, 'out of', non0, ' correct, and total dev is ', len(test_sents)\n",
    "print \"Accuracy rate is %.2f\\n\" % (correct * 1.0/ non0)                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32513 32513 1102149 1102653 2205658 1102149 1102653 2205658\n"
     ]
    }
   ],
   "source": [
    "print len(test_sentis), len(test_sents), len(test_sids), len(dev_sids), len(train_sids), len(test_ids), len(dev_ids), len(train_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
