{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade numpy\n",
    "#!pip install --upgrade nltk\n",
    "#!pip install --upgrade pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnsm' from 'rnnsm.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnsm; reload(rnnsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import rnnsm; reload(rnnsm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  sm = rnnsm.RNNSM(V=10000, Z=6, H=200, num_layers=2)\n",
    "  sm.BuildCoreGraph()\n",
    "  sm.BuildTrainGraph()\n",
    "  sm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(sm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = sm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = sm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = sm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "        h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    " \n",
    "    cost, h, _ = session.run([loss, sm.final_h_, train_op], feed_dict= {sm.target_y_: y, sm.initial_h_:h,\n",
    "        sm.input_w_: w, sm.dropout_keep_prob_:keep_prob, sm.learning_rate_:learning_rate})      \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat, 17 Dec 2016 19:38:24\n",
      "Loaded 130051 sentences (4.28041e+06 tokens)\n",
      "Loaded 130051 sentiments (130051 tokens)\n",
      "Training set: 65025 sentences (2137524 tokens)\n",
      "Test set: 32513 sentences (1072135 tokens)\n",
      "dev set: 32513 sentences (1070747 tokens)\n",
      "Training set: 65025 sentiments (65025 tokens)\n",
      "Test set: 32513 sentiments (32513 tokens)\n",
      "dev set: 32513 sentiments (32513 tokens)\n",
      "Sat, 17 Dec 2016 19:39:30\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import time\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "reload(utils)\n",
    "V = 10000\n",
    "Z = 4\n",
    "vocab, svocab, train_ids, train_sids, test_ids, test_sids, dev_ids, dev_sids, test_sents, test_sentis = \\\n",
    "    utils.load_data(\"text.full.txt\", \"sn0p.new.full.txt\", train=0.5, test=0.25, V=V, Z=Z, shuffle=True)\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  343 5635   62  145   35  532   13 5050    3   45   22   11   41   39\n",
      "  607  729    9    4    2    7  223   11 1128   62    7 1634 2865    5    0\n",
      "   22   31  360    9   40   12   93  176    3   33   31  399  388    6 6101\n",
      "   99  379   70   59  966    3  499   87   17 3996  109    5    0  470  306\n",
      "    3    9   75    7  174  615 3982    9  333  616    8  411 1576    3   96\n",
      "   12   22   32  839   14  228  395   26   40  320    3   58   24   48    7\n",
      "   14  326  219   15   96   22   32 2320  233   40]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[   0   20   11  351   13   14  194 1344    7  115    3  255 2172    8 1001\n",
      "    6   35  984  413  194   52    3  287 1321    6   43  928   12 1968    5\n",
      "    0    4  549   31 3191   26    2  210 3062    3   33  658   21   53   49\n",
      "    9 9111 1131  177  606  751   79    4   25 1231 1401 1456    3    8    2\n",
      " 1549    3  280 7392    7 9893  606    5    0   27    9  800 2066    3  133\n",
      " 1388   10  237 5026   29   14 1114  391   81    3  133  269    3   33 2556\n",
      "   24   48    7    4   25   11   86    5    0   17]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0]\n",
      "[   0    4   21   65   51  102  434 2257  229    4 1304  494   25    8 7640\n",
      " 1762    3    4  201 1982    7    4  721   81    3   28  317   10 1801    7\n",
      "  189 3533    9    4    2  147    7  201  183    5    0 1286  476   28  724\n",
      "    6  220   24   34 7792 1318   29  605 3187    3   92  170  296    6    4\n",
      "  781   18    3 1463    4  543 1197    6 2070    4  350 1375   95   14 3853\n",
      "    3    4   97   25    5    0  361 4655 1227   66   84  640  812    7   10\n",
      "  102    9   33   66   42 1188   41    6 3005   20]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ [u'Microsoft', u\"'s\", u'strategy', u'for', u'its', u'next', u'generation', u'of', u'Windows', u',', u'called', u'Vista', u'and', u'scheduled', u'to', u'be', u'released', u'early', u'next', u'year', u',', u'already', u'seems', u'to', u'have', u'raised', u'that', u'possibility', u'.']\n",
      " [u'The', u'ruling', u'was', u'requested', u'by', u'Kensington', u'International', u'Ltd.', u',', u'which', u'bought', u'$', u'275', u'million', u'in', u'Owens', u'Corning', u'bank', u'debt', u'soon', u'after', u'the', u'company', u'sought', u'bankruptcy', u'protection', u',', u'and', u'Springfield', u'Associates', u',', u'another', u'holder', u'of', u'distressed', u'debt', u'.']\n",
      " [u'As', u'in', u'past', u'quarters', u',', u'Verizon', u'got', u'a', u'big', u'boost', u'from', u'its', u'mobile', u'phone', u'group', u',', u'Verizon', u'Wireless', u',', u'which', u'generated', u'41', u'percent', u'of', u'the', u'company', u\"'s\", u'sales', u'.']\n",
      " [u'With', u'Congress', u'back', u'and', u'preparing', u'to', u'decide', u'whether', u'to', u'restore', u'or', u'cut', u'the', u'money', u',', u'officials', u'from', u'Lockheed', u'Martin', u',', u'Boeing', u'and', u'Pratt', u'&', u'Whitney', u',', u'a', u'unit', u'of', u'United', u'Technologies', u',', u'are', u'holding', u'a', u'conference', u'and', u'pep', u'rally', u'in', u'the', u'Virginia', u'suburbs', u'today', u'and', u'Wednesday', u'to', u'pump', u'up', u'top', u'subcontractors', u'of', u'the', u'F-22', u'and', u'then', u'send', u'them', u'off', u'to', u'lobby', u'Congress', u'in', u'a', u'bid', u'to', u'save', u'the', u'$', u'70', u'billion', u'program', u'.']\n",
      " [u'General', u'Motors', u'has', u'by', u'far', u'the', u'biggest', u'company', u'pension', u'plan', u'in', u'the', u'United', u'States', u',', u'having', u'promised', u'to', u'pay', u'benefits', u'worth', u'$', u'89', u'billion', u'to', u'its', u'current', u'and', u'future', u'retirees', u'as', u'of', u'the', u'end', u'of', u'2004', u'.']\n",
      " [u'Mr.', u'de', u'Icaza', u'and', u'the', u'Mono', u'team', u'based', u'its', u'system', u'on', u'public', u'specifications', u'that', u'Microsoft', u'submitted', u'to', u'an', u'international', u'standards', u'body', u',', u'a', u'typical', u'move', u'for', u'any', u'software', u'company', u'wanting', u'to', u'make', u'its', u'technology', u'a', u'standard', u'.']\n",
      " [u'To', u'avoid', u'incurring', u'a', u'$', u'400', u'million', u'breakup', u'fee', u'in', u'a', u'joint', u'venture', u'between', u'P', u'&', u'O', u'Princess', u'and', u'Royal', u'Caribbean', u'in', u'southern', u'Europe', u',', u'Carnival', u'would', u'have', u'to', u'put', u'off', u'mailing', u'formal', u'offer', u'documents', u'to', u'P', u'&', u'O', u'shareholders', u'until', u'January', u'2003', u'.']\n",
      " [u'Frustrated', u'by', u'the', u'pace', u'of', u'the', u'talks', u',', u'AT', u'&', u'T', u'last', u'year', u'began', u'the', u'process', u'of', u'turning', u'T.W.E', u'.']\n",
      " [u'Amgen', u',', u'by', u'contrast', u',', u'has', u'become', u'an', u'independent', u',', u'multinational', u'company', u',', u'on', u'par', u'with', u'some', u'of', u'the', u'more', u'conventional', u'pharmaceutical', u'companies', u'.']\n",
      " [u'The', u'investment', u'in', u'Clearwire', u'is', u'Intel', u'Capital', u\"'s\", u'biggest', u'to', u'date', u',', u'by', u'a', u'large', u'margin', u'.']\n",
      " [u'There', u'are', u'other', u'non-Microsoft', u'browsers', u',', u'like', u'Safari', u'from', u'Apple', u'Computer', u'and', u'Opera', u',', u'created', u'by', u'a', u'Norwegian', u'company', u',', u'Opera', u'Software', u'.']\n",
      " [u'In', u'fact', u',', u'AT', u'&', u'T', u'executives', u',', u'who', u'had', u'originally', u'said', u'that', u'they', u'would', u'offer', u'the', u'service', u'to', u'anyone', u'who', u'wanted', u'it', u'by', u'March', u'14', u',', u'now', u'explain', u'their', u'software', u'scarcity', u'as', u'a', u'deliberate', u'strategy', u'.']\n",
      " [u'In', u\"''Penelope\", u',', u\"''\", u'for', u'example', u',', u'Ms.', u'Cruz', u',', u'without', u'entourage', u',', u'strolls', u'into', u'a', u'California', u'beach-side', u'restaurant', u'and', u'asks', u'for', u'a', u'Coke', u'.']\n",
      " [u'While', u'the', u'company', u'is', u'expecting', u'healthy', u'profit', u'growth', u'over', u'the', u'next', u'year', u'because', u'of', u'savings', u'from', u'its', u'merger', u'with', u'Pharmacia', u',', u'Pfizer', u\"'s\", u'growth', u'prospects', u'lately', u'have', u'seemed', u'less', u'than', u'sterling', u'.']\n",
      " [u'Some', u'at', u'Microsoft', u'hoped', u'it', u'would', u'simply', u'invest', u'in', u'AOL', u',', u'but', u'keep', u'its', u'MSN', u'unit', u'intact', u'.']\n",
      " [u'Mr.', u'Levin', u'said', u'his', u'move', u'to', u'Microsoft', u'started', u'earlier', u'this', u'year', u'during', u'a', u'telephone', u'conversation', u'with', u'Rick', u'Rashid', u',', u'the', u'head', u'of', u'Microsoft', u\"'s\", u'research', u'division', u',', u'about', u'research', u'matters', u'.']\n",
      " [u'Prosecutors', u'announce', u'$', u'1.4', u'billion', u'settlement', u'with', u'10', u'of', u'nation', u\"'s\", u'biggest', u'investment', u'firms', u'and', u'2', u'well-known', u'stock', u'analysts', u';', u'deal', u'includes', u'sweeping', u'plan', u'to', u'safeguard', u'investors', u'and', u'end', u'conflict', u'of', u'interest', u';', u'it', u'details', u'far', u'greater', u'range', u'of', u'conflicts', u'of', u'interest', u'than', u'previously', u'disclosed', u',', u'and', u'leaves', u'industry', u'exposed', u'both', u'to', u'further', u'regulation', u'and', u'costly', u'litigation', u';', u'it', u'resolves', u'accusations', u'that', u'firms', u'lured', u'millions', u'of', u'investors', u'to', u'buy', u'billions', u'of', u'dollars', u'worth', u'of', u'shares', u'in', u'companies', u'they', u'knew', u'were', u'troubled', u'and', u'which', u'ultimately', u'either', u'collapsed', u'or', u'sharply', u'declined', u';', u'prosecutors', u'say', u'analysts', u'at', u'firm', u'after', u'firm', u'wittingly', u'duped', u'investors', u'to', u'curry', u'favor', u'with', u'corporate', u'clients', u';', u'say', u'investment', u'houses', u'received', u'secret', u'payments', u'from', u'companies', u'they', u'gave', u'strong', u'recommendations', u'to', u'buy', u';', u'say', u'for', u'top', u'executives', u'whose', u'companies', u'were', u'clients', u',', u'stock', u'underwriters', u'offered', u'special', u'access', u'to', u'hot', u'initial', u'public', u'offerings', u';', u'regulators', u'find', u'fault', u'with', u'every', u'major', u'bank', u'on', u'Wall', u'St', u'and', u'single', u'out', u'for', u'charges', u'of', u'fraud', u'Citigroup', u\"'s\", u'Salomon', u'Smith', u'Barney', u',', u'Merrill', u'Lynch', u'and', u'Credit', u'Suisse', u'First', u'Boston', u';', u'firms', u'accused', u'of', u'making', u'unwarranted', u'or', u'exaggerated', u'claims', u'about', u'companies', u'they', u'analyzed', u'are', u'Bear', u'Stearns', u',', u'Goldman', u'Sachs', u',', u'Lehman', u'Bros', u',', u'Piper', u'Jaffray', u'and', u'UBS', u'Warburg', u';', u'two', u'analysts', u',', u'Jack', u'B', u'Grubman', u'of', u'Salomon', u'Smith', u'Barney', u'and', u'Henry', u'Blodget', u'of', u'Merrill', u'Lynch', u',', u'agree', u'to', u'lifetime', u'bans', u'from', u'industry', u',', u'along', u'with', u'significant', u'fines', u';', u'Citigroup', u\"'s\", u'chief', u'executive', u'Sanford', u'I', u'Weill', u'is', u'barred', u'from', u'communicating', u'with', u'his', u'firm', u\"'s\", u'stock', u'analysts', u'about', u'companies', u'they', u'cover', u'unless', u'lawyer', u'is', u'present', u';', u'photos', u';', u'chart', u'(', u'L', u')']\n",
      " [u'On', u'a', u'special', u'Web', u'page', u'put', u'up', u'for', u'the', u'occasion', u',', u'I.B.M', u'.']\n",
      " [u'Moreover', u',', u'Microsoft', u\"'s\", u'announcement', u'today', u'showed', u'its', u'ambitions', u'to', u'take', u'Web-browser', u'software', u'beyond', u'a', u'navigational', u'tool', u'by', u'building', u'in', u'a', u'package', u'of', u'popular', u'destinations', u'.']\n",
      " [u'Sir', u'Deryck', u',', u'54', u',', u'had', u'earlier', u'been', u'chief', u'executive', u'of', u'Salomon', u'Brothers', u',', u'which', u'was', u'acquired', u'by', u'a', u'predecessor', u'to', u'Citigroup', u'in', u'1997', u'.']\n",
      " [u'George', u'E.', u'Pataki', u'said', u'it', u'would', u'be', u'the', u'largest', u'single', u'industrial', u'investment', u'in', u'the', u'state', u\"'s\", u'history', u',', u'surpassing', u'a', u'1994', u'joint', u'venture', u'in', u'the', u'same', u'town', u'that', u'also', u'involved', u'I.B.M', u'.']\n",
      " [u'In', u'recent', u'months', u',', u'several', u'consumer', u'on-line', u'services', u'have', u'either', u'failed', u'or', u'abandoned', u'their', u'proprietary', u'approach', u'in', u'favor', u'of', u'becoming', u'open', u'services', u'within', u'the', u'Internet', u',', u'including', u'the', u'News', u'Corporation', u\"'s\", u'Delphi', u',', u'the', u'General', u'Electric', u'Company', u\"'s\", u'Genie', u',', u'AT', u'&', u'T', u\"'s\", u'Interchange', u',', u'and', u'the', u'Microsoft', u'Corporation', u\"'s\", u'MSN', u'.']\n",
      " [u'General', u'Motors', u'Corp', u'will', u'extend', u'its', u'Keep', u'America', u'Rolling', u'incentive', u'package', u'and', u'continue', u'to', u'offer', u'interest-free', u'financing', u'through', u'Jan', u'2', u',', u'but', u'new', u'phase', u'of', u'plan', u'will', u'be', u'less', u'generous', u'than', u'current', u'one', u'and', u'will', u'not', u'include', u'Cadillacs', u',', u'Corvettes', u'and', u'some', u'Saturn', u'vehicles', u'(', u'M', u')']\n",
      " [u'Wal-Mart', u'declined', u'to', u'say', u'how', u'much', u'the', u'changes', u'would', u'cost', u',', u'a', u'number', u'that', u'investors', u'will', u'be', u'eager', u'to', u'learn', u',', u'given', u'Wal-Mart', u\"'s\", u'no-frills', u'business', u'model', u',', u'which', u'emphasizes', u'low', u'labor', u'costs', u'.']\n",
      " [u'plans', u'today', u'to', u'announce', u'the', u'formation', u'of', u'a', u'500-member', u'consulting', u'group', u'dedicated', u'to', u'the', u'Microsoft', u'Windows', u'NT', u'operating', u'system', u'and', u'business-oriented', u'applications', u'.']\n",
      " [u'Under', u'the', u'agreement', u',', u'Wayport', u'plans', u'to', u'offer', u'Wi-Fi', u'service', u'in', u'as', u'many', u'as', u'3,000', u'McDonald', u\"'s\", u'restaurants', u'by', u'the', u'end', u'of', u'the', u'year', u',', u'charging', u'$', u'2.95', u'for', u'two', u'hours', u'of', u'access', u'.']\n",
      " [u'Time', u'Warner', u'would', u'also', u'contribute', u'to', u'the', u'settlement', u',', u'the', u'Attorney', u'General', u\"'s\", u'office', u'said', u'.']\n",
      " [u'For', u'nearly', u'four', u'weeks', u'after', u'the', u'Blaster', u'attack', u',', u'Microsoft', u'said', u',', u'its', u'sales', u'staff', u'was', u'preoccupied', u'with', u'customers', u\"'\", u'security', u'concerns', u'.']\n",
      " [u'South', u'Korea', u\"'s\", u'commerce', u'and', u'industry', u'minister', u'said', u'that', u'General', u'Motors', u'had', u'until', u'the', u'end', u'of', u'April', u'to', u'decide', u'if', u'it', u'wanted', u'to', u'buy', u'the', u'Daewoo', u'Motor', u'Company', u'.']\n",
      " [u'Yahoo', u'called', u'Mr.', u'Braun', u'the', u'right', u'executive', u'to', u'bring', u'a', u'new', u'creative', u'dimension', u'to', u'Yahoo', u\"'s\", u'plans', u'to', u'provide', u'Web', u'users', u'with', u'services', u'in', u'areas', u'including', u'news', u',', u'sports', u',', u'health', u',', u'finance', u'and', u'games', u',', u'as', u'well', u'as', u'future', u'areas', u'of', u'entertainment', u'.']]\n",
      "[['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['n']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['n']\n",
      " ['p']\n",
      " ['p']\n",
      " ['p']]\n",
      "2202550 2202550 1104649 1104649 1103261 1103261\n"
     ]
    }
   ],
   "source": [
    "print train_ids[:100]\n",
    "print train_sids[:100]\n",
    "print test_ids[:100]\n",
    "print test_sids[:100]\n",
    "print dev_ids[:100]\n",
    "print dev_sids[:100]\n",
    "print test_sents[:30]\n",
    "print test_sentis[:30]\n",
    "print len(train_ids), len(train_sids), len(test_ids), len(test_sids), len(dev_ids), len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100,\n",
    "                    Z=Z,\n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = './tf_saved/tf_saved_rnnsm_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(sm, session, ids, sids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, sids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(sm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed, 14 Dec 2016 07:26:53\n",
      "WARNING:tensorflow:From <ipython-input-9-66f68d545e13>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-66f68d545e13>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:31:29\n",
      "[epoch 1] Completed in 0:04:34\n",
      "[epoch 1] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.726  (perplexity: 2.07)\n",
      "[epoch 1] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.722  (perplexity: 2.06)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:37:17\n",
      "[epoch 2] Completed in 0:04:32\n",
      "[epoch 2] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.718  (perplexity: 2.05)\n",
      "[epoch 2] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.717  (perplexity: 2.05)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:43:07\n",
      "[epoch 3] Completed in 0:04:36\n",
      "[epoch 3] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.713  (perplexity: 2.04)\n",
      "[epoch 3] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.715  (perplexity: 2.04)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:48:55\n",
      "[epoch 4] Completed in 0:04:30\n",
      "[epoch 4] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.708  (perplexity: 2.03)\n",
      "[epoch 4] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.713  (perplexity: 2.04)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "in batch_generator 2206693 2206693 2206650 2206650 2206650 50\n",
      "Wed, 14 Dec 2016 07:54:43\n",
      "[epoch 5] Completed in 0:04:34\n",
      "[epoch 5] in batch_generator 2206693 2206693 2206600 2206600 2206600 100\n",
      "Train set: avg. loss: 0.703  (perplexity: 2.02)\n",
      "[epoch 5] in batch_generator 1100956 1100956 1100900 1100900 1100900 100\n",
      "Test set: avg. loss: 0.713  (perplexity: 2.04)\n",
      "\n",
      "Wed, 14 Dec 2016 07:55:58\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "reload(utils)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  os.environ['TZ'] = 'US/Pacific'\n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    sm = rnnsm.RNNSM(**model_params)\n",
    "    sm.BuildCoreGraph()\n",
    "    sm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, train_sids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    run_epoch(sm, session, bi, train=True, keep_prob=keep_prob, learning_rate=learning_rate)\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, train_ids, train_sids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, dev_ids, dev_sids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, './tf_saved/tf_saved_rnnsm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)\n",
    "  \n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_step(sm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    sm : rnnsm.RNNSM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  final_h, samples = session.run([sm.final_h_, sm.pred_samples_], \n",
    "        feed_dict={sm.input_w_: input_w, sm.initial_h_: initial_h, sm.dropout_keep_prob_: 1.0, sm.learning_rate_:0.1})\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#skip unless mixed\n",
    "# Same as above, but as a batch\n",
    "import collections\n",
    "reload(rnnsm)\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  tf.set_random_seed(random_seed)\n",
    "\n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    sm = rnnsm.RNNSM(**model_params)\n",
    "    sm.BuildCoreGraph()\n",
    "    sm.BuildSamplerGraph()\n",
    "\n",
    "  # Load the trained model\n",
    "  saver = tf.train.Saver()\n",
    "  saver.restore(session, trained_filename)\n",
    "\n",
    "  # Make initial state for a batch with batch_size = num_samples\n",
    "  w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "  #print w\n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  # We'll take one step for each sequence on each iteration \n",
    "  for i in xrange(max_steps):\n",
    "    h, y = sample_step(sm, session, w[:,-1:], h)\n",
    "    w = np.hstack((w,y))\n",
    "  #print w\n",
    "  svocab=vocabulary.Vocabulary(True)\n",
    "  for row in w:\n",
    "    k = [sword_id for i, sword_id in enumerate(row)]\n",
    "    k = [svocab.id_to_word[sword_id] for sword_id in k]\n",
    "    #print collections.Counter(k)\n",
    "    #print svocab.id_to_word[sword_id],\n",
    "  #collections.Counter([svocab.id_to_word(sword_id) for i, sword_id in k])\n",
    "        \n",
    "  # Print generated sentences\n",
    "  #for row in w:\n",
    "  #  for i, word_id in enumerate(row):\n",
    "  #    print vocab.id_to_word[word_id],\n",
    "  #    if (i != 0) and (word_id == vocab.START_ID):\n",
    "  #      break\n",
    "  #  print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_predict(sm, session, seq, vocab, svocab):\n",
    "  \"\"\"Score by test_ids vs test_sids\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  h, y = sample_step(sm, session, w[:,-1:], h)\n",
    "\n",
    "  y = [1 if k == 3 else k for k in utils.flatten(y)]\n",
    "\n",
    "  #return [svocab.ids_to_words(k) for k in y]\n",
    "  return svocab.ids_to_words(y)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15396\n",
      "Accuracy rate is 0.47\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:  \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        sm = rnnsm.RNNSM(**model_params)\n",
    "        sm.BuildCoreGraph()\n",
    "        sm.BuildSamplerGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './'+trained_filename)\n",
    "    pred = []\n",
    "    for s in test_sents:\n",
    "        pred.append(seq_predict(sm, session, s, vocab, svocab))\n",
    "                        \n",
    "    correct = 0\n",
    "    for i in range(len(test_sents)):\n",
    "        if pred[i] == test_sentis[i][0]:\n",
    "            correct = correct + 1\n",
    "    print correct\n",
    "print \"Accuracy rate is %.2f\\n\" % (correct * 1.0/ len(test_sents))                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
