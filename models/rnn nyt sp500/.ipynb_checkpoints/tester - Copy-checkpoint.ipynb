{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnsm' from 'rnnsm.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnsm; reload(rnnsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57340 sentences (1.16119e+06 tokens)\n",
      "Training set: 45872 sentences (979646 tokens)\n",
      "Test set: 11468 sentences (181546 tokens)\n"
     ]
    }
   ],
   "source": [
    "corpus = utils.get_corpus('brown')\n",
    "senti = [int(line[:-2]) if len(line[:-2])!=0 else int(line[:]) for line in open('./senti.txt')]\n",
    "vocab = utils.build_vocab(corpus, 10000)\n",
    "train_sentences, test_sentences = utils.get_train_test_sents(corpus, 0.8, 0)\n",
    "train_ids = utils.preprocess_sentences(train_sentences, vocab)\n",
    "#train_target = \n",
    "test_ids = utils.preprocess_sentences(test_sentences, vocab)\n",
    "#test_target =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025519 193015 45872 11468 1161192 57340\n"
     ]
    }
   ],
   "source": [
    "print len(train_ids), len(test_ids), len(train_sentences), len(test_sentences), len(corpus.words()), len(corpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corpus.sents()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print train_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 155941 sentences (4.43612e+06 tokens)\n",
      "Training set: 124752 sentences (3525140 tokens)\n",
      "Test set: 31189 sentences (910981 tokens)\n"
     ]
    }
   ],
   "source": [
    "scorpus = utils.get_corpus('text.txt')\n",
    "senti = [int(line[:-2]) if len(line[:-2])!=0 else int(line[:]) for line in open('./senti.txt')]\n",
    "svocab = utils.build_vocab(scorpus, 10000)\n",
    "strain_sentences, stest_sentences = utils.get_train_test_sents(scorpus, 0.8, 0)\n",
    "strain_ids = utils.preprocess_sentences(strain_sentences, svocab)\n",
    "#train_target = \n",
    "stest_ids = utils.preprocess_sentences(stest_sentences, svocab)\n",
    "#test_target =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Was', u'last', u'month', u'the', u'peak', u'?']\n",
      " [u'The', u'wave', u'of', u'buyouts', u'continued', u'to', u'rise', u'in', u'May', u',', u'making', u'it', u'the', u'one', u'of', u'the', u'biggest', u'deal', u'-', u'making', u'months', u'ever', u'.']\n",
      " [u'Only', u'January', u'2000', u',', u'when', u'the', u'America', u'Online', u'-', u'Time', u'Warner', u'merger', u'was', u'announced', u',', u'and', u'April', u'1998', u'(', u'the', u'Citicorp', u'-', u'Travelers', u'deal', u')', u'were', u'bigger', u'..', u'Dow', u'Jones', u',', u'which', u'owns', u'The', u'Wall', u'Street', u'Journal', u'and', u'other', u'media', u'properties', u',', u'is', u'entertaining', u'a', u'$', u'5', u'billion', u'takeover', u'offer', u'from', u'Rupert', u'Murdoch', u',', u'who', u'controls', u'the', u'News', u'Corporation', u'.']\n",
      " [u'The', u'Bancroft', u'family', u',', u'which', u'controls', u'Dow', u'Jones', u',', u'met', u'with', u'Mr', u'.', u'Murdoch', u'this', u'week', u'to', u'discuss', u'terms', u',', u'and', u'a', u'few', u'other', u'potential', u'suitors', u'have', u'stepped', u'forward', u'to', u'express', u'interest', u'in', u'bidding', u'for', u'the', u'company', u'.']\n",
      " [u'Without', u'the', u'taxes', u'being', u'paid', u',', u'Richard', u'F', u'.', u'Zannino', u',', u'who', u'became', u'the', u'chief', u'executive', u'of', u'Dow', u'Jones', u'in', u'February', u'2006', u',', u'could', u'be', u'eligible', u'for', u'more', u'than', u'$', u'19', u'.', u'7', u'million', u'in', u'severance', u',', u'retirement', u'benefits', u'and', u'stock', u'and', u'option', u'payouts', u'based', u'on', u'the', u'$', u'60', u'-', u'a', u'-', u'share', u'price', u'offered', u'by', u'the', u'News', u'Corporation', u'.']\n",
      " [u'L', u'.', u'Gordon', u'Crovitz', u',', u'The', u'Wall', u'Street', u'Journal', u\"'\", u's', u'publisher', u',', u'could', u'be', u'eligible', u'for', u'more', u'than', u'$', u'7', u'.', u'2', u'million', u'.']\n",
      " [u'The', u'Vodafone', u'Group', u',', u'the', u'world', u\"'\", u's', u'largest', u'cellphone', u'company', u',', u'said', u'on', u'Thursday', u'that', u'it', u'had', u'received', u'a', u'letter', u'from', u'a', u'group', u'of', u'shareholders', u'asking', u'it', u'to', u'return', u'as', u'much', u'as', u'\\xa3', u'38', u'billion', u'($', u'75', u'billion', u')', u'to', u'investors', u'in', u'part', u'by', u'spinning', u'off', u'its', u'stake', u'in', u'Verizon', u'Wireless', u'.']\n",
      " [u'In', u'the', u'latest', u'effort', u'by', u'minority', u'shareholders', u'to', u'influence', u'management', u'decisions', u',', u'a', u'little', u'-', u'known', u'group', u'of', u'investors', u',', u'Efficient', u'Capital', u'Structures', u',', u'suggested', u'that', u'Vodafone', u'give', u'its', u'investors', u'tracking', u'shares', u'representing', u'the', u'company', u\"'\", u's', u'45', u'percent', u'stake', u'in', u'Verizon', u'Wireless', u',', u'the', u'American', u'cellular', u'carrier', u';', u'increase', u'borrowing', u'as', u'a', u'way', u'to', u'return', u'more', u'money', u'to', u'shareholders', u';', u'and', u'ask', u'the', u'permission', u'of', u'shareholders', u'before', u'making', u'any', u'large', u'acquisitions', u'.']\n",
      " [u'An', u'analyst', u'at', u'Goldman', u'Sachs', u'in', u'London', u',', u'Simon', u'Weeden', u',', u'said', u'that', u'he', u'did', u'not', u'expect', u'the', u'claims', u'to', u'be', u'taken', u'seriously', u'and', u'that', u'a', u'spinoff', u'of', u'Vodafone', u\"'\", u's', u'stake', u'in', u'Verizon', u'Wireless', u'would', u'create', u'complex', u'tax', u'issues', u'and', u'would', u'dilute', u'shareholder', u'rights', u'.']\n",
      " [u'Contrast', u'that', u'with', u'the', u'success', u'of', u'the', u'Wii', u'.']]\n"
     ]
    }
   ],
   "source": [
    "print strain_sentences[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3649893 942171 124752 31189 4435274 155941\n"
     ]
    }
   ],
   "source": [
    "print len(strain_ids), len(stest_ids), len(strain_sentences), len(stest_sentences), len(scorpus.words()), len(scorpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss =  strain_sentences[0:3]\n",
    "ww =  stest_sentences[0:3]\n",
    "open('sss.txt','w').write(ss)\n",
    "open('www.txt','w').write(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\xd8r[\\xd3\\x97\\x7f\\x00\\x00\\x80\\x7f[\\xd3\\x97\\x7f\\x00\\x00\\x18~[\\xd3\\x97\\x7f\\x00\\x00']\n"
     ]
    }
   ],
   "source": [
    "print [l for l in open('sss.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'Was', u'last', u'month', u'the', u'peak', u'?']\n",
      " [u'The', u'wave', u'of', u'buyouts', u'continued', u'to', u'rise', u'in', u'May', u',', u'making', u'it', u'the', u'one', u'of', u'the', u'biggest', u'deal', u'-', u'making', u'months', u'ever', u'.']\n",
      " [u'Only', u'January', u'2000', u',', u'when', u'the', u'America', u'Online', u'-', u'Time', u'Warner', u'merger', u'was', u'announced', u',', u'and', u'April', u'1998', u'(', u'the', u'Citicorp', u'-', u'Travelers', u'deal', u')', u'were', u'bigger', u'..', u'Dow', u'Jones', u',', u'which', u'owns', u'The', u'Wall', u'Street', u'Journal', u'and', u'other', u'media', u'properties', u',', u'is', u'entertaining', u'a', u'$', u'5', u'billion', u'takeover', u'offer', u'from', u'Rupert', u'Murdoch', u',', u'who', u'controls', u'the', u'News', u'Corporation', u'.']]\n"
     ]
    }
   ],
   "source": [
    "print ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ccc = nltk.corpus.PlaintextCorpusReader('./', 'tmp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'To shareholders of Tyco International , the industrial products conglomerate , deals are usually no big deal .', u'The company , which makes ADT security systems and electronic components for hospitals , cars and computers , has made four purchases worth about $ 4 billion in the last four months alone and has long had an acquisitive streak .']\n"
     ]
    }
   ],
   "source": [
    "#print ' '.join(ccc.sents()[0])\n",
    "\n",
    "#print ccc.sents()\n",
    "#print ccc.raw()\n",
    "sss = [' '.join(k) for k in ccc.sents()]\n",
    "print sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:3: DeprecationWarning: the sets module is deprecated\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'togds.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8f9b41bf67a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mftogds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"togds.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'togds.txt'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from sets import Set\n",
    "\n",
    "ftogds = open(\"togds.txt\", \"r\")\n",
    "\n",
    "dto = {}\n",
    "dgs ={}\n",
    "\n",
    "for line in ftogds:\n",
    "    fields = line.split(',')\n",
    "    dto[fields[0]] = Set([])\n",
    "\n",
    "ftogds.close()\n",
    "\n",
    "ftogds = open(\"togds.txt\", \"r\")\n",
    "\n",
    "for line in ftogds:\n",
    "    fields = line.split(',')\n",
    "    oname = fields[1].encode('utf-8').strip().lower()\n",
    "    if oname.split(' ')[-1][0:2] == 'in' or oname.split(' ')[-1][0:2] == 'co':\n",
    "        #cut from end to the last ' '\n",
    "        oname = ' '.join(oname.split(' ')[:-1])\n",
    "    dto[fields[0]].add(oname)\n",
    "    sss = fields[4]\n",
    "    if sss[-1]=='\\n':\n",
    "        sss = sss[:-1]\n",
    "    dgs[fields[2]]=str(int(float(sss)*1000000000.0)/12000000+37)\n",
    "ftogds.close()\n",
    "\n",
    "ftodgs = open(\"togds.txt\", 'r')\n",
    "fall = open('all.txt', 'w')\n",
    "\n",
    "for line in ftodgs:\n",
    "    fields = line.split(',')\n",
    "    ticker  = fields[0]\n",
    "    guid = fields[2]\n",
    "    \n",
    "    fin = 'nyt'+'/'+str(guid)+'.xml'\n",
    "    f = open(fin, 'r')\n",
    "\n",
    "    '''\n",
    "    this is the final step, well, almost, before the merge\n",
    "    '''\n",
    "    tree = ET.parse(fin)\n",
    "\n",
    "    for block in tree.getroot().iter('block'):\n",
    "        if block.get('class') == 'full_text':\n",
    "            for p in block.iter('p'):\n",
    "                \n",
    "                g = open('./tmp.txt','w')\n",
    "                g.write(p.text+'\\n')\n",
    "                g.close()\n",
    "                corpus = nltk.corpus.PlaintextCorpusReader('./', 'tmp.txt')\n",
    "                sentences = [' '.join(k) for k in corpus.sents()]\n",
    "                for sentence in sentences:\n",
    "                    done = False\n",
    "                    for o in dto[ticker]:\n",
    "                        if o.strip().lower() in sentence.strip().lower() and not done:\n",
    "                            fall.write(dgs[guid]+'|'+sentence.encode('utf-8')+'\\n')\n",
    "                            done = True\n",
    "    f.close()\n",
    "\n",
    "fall.close()\n",
    "ftogds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
