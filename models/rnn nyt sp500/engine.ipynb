{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The RNN/LSTM on NYT with SP500 running trend as the sentiment indicator\n",
    "\n",
    "This is an implementation based on the rnn/lstm model we used in Assignment 1, Part 2, adapted to predict and learn to associate each word of a sentence to its corresponding stock trend.  In this case a stock trend will be 'p' if the SP500 mention company's current weekly average price rised compared to the previous week  \n",
    "\n",
    "### Model Level Description\n",
    "\n",
    "in utility.py, I switched the corpus loading from 'brown' to 'text.full.txt', the NYT 130051 sents which has a mention to one of the SP500 company.\n",
    "\n",
    "\n",
    "![RNNLM - layers](RNNLM - layers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnsm' from 'rnnsm.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time, shutil\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils; reload(utils)\n",
    "import vocabulary; reload(vocabulary)\n",
    "import rnnsm; reload(rnnsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-1cf394c21277>:17 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-1cf394c21277>:17 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import rnnsm; reload(rnnsm)\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  tf.set_random_seed(42)\n",
    "\n",
    "  sm = rnnsm.RNNSM(V=10000, Z=4, H=200, num_layers=2)\n",
    "  sm.BuildCoreGraph()\n",
    "  sm.BuildTrainGraph()\n",
    "  sm.BuildSamplerGraph()\n",
    "\n",
    "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                          tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(sm, session, batch_iterator, train=False,\n",
    "              verbose=False, tick_s=10, \n",
    "              keep_prob=1.0, learning_rate=0.1):\n",
    "  start_time = time.time()\n",
    "  tick_time = start_time  # for showing status\n",
    "  total_cost = 0.0  # total cost, summed over all words\n",
    "  total_words = 0\n",
    "\n",
    "  if train:\n",
    "    train_op = sm.train_step_\n",
    "    keep_prob = keep_prob\n",
    "    loss = sm.train_loss_\n",
    "  else:\n",
    "    train_op = tf.no_op()\n",
    "    keep_prob = 1.0  # no dropout at test time\n",
    "    loss = sm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "  for i, (w, y) in enumerate(batch_iterator):\n",
    "    cost = 0.0\n",
    "    #### YOUR CODE HERE ####\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # At first batch in epoch, get a clean intitial state\n",
    "    if i == 0:\n",
    "        h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    " \n",
    "    cost, h, _ = session.run([loss, sm.final_h_, train_op], feed_dict= {sm.target_y_: y, sm.initial_h_:h,\n",
    "        sm.input_w_: w, sm.dropout_keep_prob_:keep_prob, sm.learning_rate_:learning_rate})      \n",
    "    \n",
    "    #### END(YOUR CODE) ####\n",
    "    total_cost += cost\n",
    "    total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "    ##\n",
    "    # Print average loss-so-far for epoch\n",
    "    # If using train_loss_, this may be an underestimate.\n",
    "    if verbose and (time.time() - tick_time >= tick_s):\n",
    "      avg_cost = total_cost / total_words\n",
    "      avg_wps = total_words / (time.time() - start_time)\n",
    "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
    "          total_words, avg_wps, avg_cost)\n",
    "      tick_time = time.time()  # reset time ticker\n",
    "\n",
    "  return total_cost / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun, 18 Dec 2016 07:01:38\n",
      "Loaded 130051 sentences (4.28041e+06 tokens)\n",
      "Loaded 130051 sentiments (130051 tokens)\n",
      "Training set: 65025 sentences (2141129 tokens)\n",
      "Test set: 32513 sentences (1072187 tokens)\n",
      "dev set: 32513 sentences (1067090 tokens)\n",
      "Training set: 65025 sentiments (65025 tokens)\n",
      "Test set: 32513 sentiments (32513 tokens)\n",
      "dev set: 32513 sentiments (32513 tokens)\n",
      "Sun, 18 Dec 2016 07:02:18\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import time\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "\n",
    "np.random.seed(168)\n",
    "reload(utils)\n",
    "V = 10000\n",
    "Z = 4\n",
    "vocab, svocab, train_ids, train_sids, test_ids, test_sids, dev_ids, dev_sids, test_sents, test_sentis = \\\n",
    "    utils.load_data(\"text.full.txt\", \"sn0p.full.txt\", train=0.5, test=0.25, V=V, Z=Z, shuffle=True)\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 79, 'n': 15738, 'p': 16696})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter([i[0] for i in test_sentis]) # for lm approach model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "keep_prob = 1.0\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=V, \n",
    "                    H=100,\n",
    "                    Z=Z,\n",
    "                    num_layers=1)\n",
    "\n",
    "trained_filename = './tf_saved/tf_saved_rnnsm_trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dataset(sm, session, ids, sids, name=\"Data\"):\n",
    "  bi = utils.batch_generator(ids, sids, batch_size=100, max_time=100)\n",
    "  cost = run_epoch(sm, session, bi, \n",
    "                   learning_rate=1.0, keep_prob=1.0, \n",
    "                   train=False, verbose=False, tick_s=3600)\n",
    "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun, 18 Dec 2016 07:02:34\n",
      "WARNING:tensorflow:From <ipython-input-20-26d33fab470b>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-26d33fab470b>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:06:34\n",
      "[epoch 1] Completed in 0:03:58\n",
      "[epoch 1] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.719  (perplexity: 2.05)\n",
      "[epoch 1] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.717  (perplexity: 2.05)\n",
      "\n",
      "[epoch 2] Starting epoch 2\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:11:34\n",
      "[epoch 2] Completed in 0:03:57\n",
      "[epoch 2] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.713  (perplexity: 2.04)\n",
      "[epoch 2] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.716  (perplexity: 2.05)\n",
      "\n",
      "[epoch 3] Starting epoch 3\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:16:29\n",
      "[epoch 3] Completed in 0:03:53\n",
      "[epoch 3] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.709  (perplexity: 2.03)\n",
      "[epoch 3] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.717  (perplexity: 2.05)\n",
      "\n",
      "[epoch 4] Starting epoch 4\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:21:26\n",
      "[epoch 4] Completed in 0:03:55\n",
      "[epoch 4] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.700  (perplexity: 2.01)\n",
      "[epoch 4] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.718  (perplexity: 2.05)\n",
      "\n",
      "[epoch 5] Starting epoch 5\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:26:15\n",
      "[epoch 5] Completed in 0:03:46\n",
      "[epoch 5] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.689  (perplexity: 1.99)\n",
      "[epoch 5] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.720  (perplexity: 2.05)\n",
      "\n",
      "[epoch 6] Starting epoch 6\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:31:19\n",
      "[epoch 6] Completed in 0:03:55\n",
      "[epoch 6] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.679  (perplexity: 1.97)\n",
      "[epoch 6] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.729  (perplexity: 2.07)\n",
      "\n",
      "[epoch 7] Starting epoch 7\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:36:07\n",
      "[epoch 7] Completed in 0:03:45\n",
      "[epoch 7] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.665  (perplexity: 1.94)\n",
      "[epoch 7] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.731  (perplexity: 2.08)\n",
      "\n",
      "[epoch 8] Starting epoch 8\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:40:51\n",
      "[epoch 8] Completed in 0:03:40\n",
      "[epoch 8] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.658  (perplexity: 1.93)\n",
      "[epoch 8] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.732  (perplexity: 2.08)\n",
      "\n",
      "[epoch 9] Starting epoch 9\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:45:28\n",
      "[epoch 9] Completed in 0:03:36\n",
      "[epoch 9] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.643  (perplexity: 1.90)\n",
      "[epoch 9] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.748  (perplexity: 2.11)\n",
      "\n",
      "[epoch 10] Starting epoch 10\n",
      "in batch_generator 2206155 2206155 2206150 2206150 2206150 50\n",
      "Sun, 18 Dec 2016 07:50:04\n",
      "[epoch 10] Completed in 0:03:37\n",
      "[epoch 10] in batch_generator 2206155 2206155 2206100 2206100 2206100 100\n",
      "Train set: avg. loss: 0.629  (perplexity: 1.88)\n",
      "[epoch 10] in batch_generator 1099604 1099604 1099600 1099600 1099600 100\n",
      "Test set: avg. loss: 0.751  (perplexity: 2.12)\n",
      "\n",
      "Sun, 18 Dec 2016 07:51:04\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "reload(utils)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  # Seed RNG for repeatability\n",
    "  os.environ['TZ'] = 'US/Pacific'\n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "  tf.set_random_seed(42)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None):\n",
    "    sm = rnnsm.RNNSM(**model_params)\n",
    "    sm.BuildCoreGraph()\n",
    "    sm.BuildTrainGraph()\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.batch_generator(train_ids, train_sids, batch_size, max_time)\n",
    "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "    #### YOUR CODE HERE ####\n",
    "\n",
    "    run_epoch(sm, session, bi, train=True, keep_prob=keep_prob, learning_rate=learning_rate)\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "    ##\n",
    "    # score_dataset will run a forward pass over the entire dataset\n",
    "    # and report perplexity scores. This can be slow (around 1/2 to \n",
    "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "    # to speed up training on a slow machine. Be sure to run it at the \n",
    "    # end to evaluate your score.\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, train_ids, train_sids, name=\"Train set\")\n",
    "    print (\"[epoch %d]\" % epoch),\n",
    "    score_dataset(sm, session, dev_ids, dev_sids, name=\"Test set\")\n",
    "    print \"\"\n",
    "    \n",
    "    # Save a checkpoint\n",
    "    saver.save(session, './tf_saved/tf_saved_rnnsm', global_step=epoch)\n",
    "    \n",
    "  # Save final model\n",
    "  saver.save(session, trained_filename)\n",
    "  \n",
    "  print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_step(sm, session, input_w, initial_h):\n",
    "  \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "  Args:\n",
    "    sm : rnnsm.RNNSM\n",
    "    session: tf.Session\n",
    "    input_w : [batch_size] list of indices\n",
    "    initial_h : [batch_size, hidden_dims]\n",
    "  \n",
    "  Returns:\n",
    "    final_h : final hidden state, compatible with initial_h\n",
    "    samples : [batch_size, 1] vector of indices\n",
    "  \"\"\"\n",
    "  #### YOUR CODE HERE ####\n",
    "  # Reshape input to column vector\n",
    "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "  # Run sample ops\n",
    "  final_h, samples = session.run([sm.final_h_, sm.pred_samples_], \n",
    "        feed_dict={sm.input_w_: input_w, sm.initial_h_: initial_h, sm.dropout_keep_prob_: 1.0, sm.learning_rate_:0.1})\n",
    "  \n",
    "  #### END(YOUR CODE) ####\n",
    "  return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seq_predict(sm, session, seq, vocab, svocab):\n",
    "  \"\"\"Score by test_ids vs test_sids\"\"\"\n",
    "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
    "                                                           wordset=vocab.word_to_id))\n",
    "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "  h = session.run(sm.initial_h_, {sm.input_w_: w})\n",
    "  h, y = sample_step(sm, session, w[:,-1:], h)\n",
    "\n",
    "  y = [1 if k == 3 else k for k in utils.flatten(y)]\n",
    "\n",
    "  #return [svocab.ids_to_words(k) for k in y]\n",
    "  return svocab.ids_to_words(y)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: 15894 out of 32434  correct, and total dev is  32513\n",
      "Accuracy rate is 0.49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    tf.set_random_seed(168)\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        sm = rnnsm.RNNSM(**model_params)\n",
    "        sm.BuildCoreGraph()\n",
    "        sm.BuildSamplerGraph()\n",
    "        \n",
    "    # Load the trained model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(session, './'+trained_filename)\n",
    "    pred = []\n",
    "\n",
    "    for s in test_sents:\n",
    "        pred.append(seq_predict(sm, session, s, vocab, svocab))\n",
    "\n",
    "    non0 = 0\n",
    "    correct = 0\n",
    "    for i in range(len(test_sents)):\n",
    "        if not test_sentis[i][0] == '0':\n",
    "            non0 = non0 + 1\n",
    "            if pred[i] == test_sentis[i][0]:\n",
    "                correct = correct + 1\n",
    "print \"Test result:\", correct, 'out of', non0, ' correct, and total dev is ', len(test_sents)\n",
    "print \"Accuracy rate is %.2f\\n\" % (correct * 1.0/ non0)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206155 1099604 1104701 32513 32513 4410460\n"
     ]
    }
   ],
   "source": [
    "print len(train_sids), len(dev_ids), len(test_sids), len(test_sents), len(test_sentis), (len(train_sids)+len(dev_sids)+len(test_sids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun, 18 Dec 2016 08:09:46\n",
      "Loaded 130051 sentences (4.28041e+06 tokens)\n",
      "Loaded 130051 sentiments (130051 tokens)\n",
      "Training set: 65025 sentences (2141129 tokens)\n",
      "Test set: 32513 sentences (1072187 tokens)\n",
      "dev set: 32513 sentences (1067090 tokens)\n",
      "Training set: 65025 sentiments (65025 tokens)\n",
      "Test set: 32513 sentiments (32513 tokens)\n",
      "dev set: 32513 sentiments (32513 tokens)\n",
      "Sun, 18 Dec 2016 08:10:28\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import time\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "\n",
    "np.random.seed(168)\n",
    "reload(utils)\n",
    "V = 10000\n",
    "Z = 4\n",
    "vocab, svocab, train_ids, train_sids, test_ids, test_sids, dev_ids, dev_sids, train_sents, train_sentis, test_sents, test_sentis, dev_sents, dev_sentis = \\\n",
    "    utils.load_data(\"text.full.txt\", \"sn0p.full.txt\", train=0.5, test=0.25, V=V, Z=Z, shuffle=True)\n",
    "print time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206155 1099604 1104701 32513 32513 4410460\n"
     ]
    }
   ],
   "source": [
    "print len(train_sids), len(dev_ids), len(test_sids), len(test_sents), len(test_sentis), (len(train_sids)+len(dev_sids)+len(test_sids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'p': 16696, 'n': 15738, '0': 79})\n"
     ]
    }
   ],
   "source": [
    "print collections.Counter([i[0] for i in test_sentis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'p': 33780, 'n': 31068, '0': 177})\n",
      "Counter({'p': 16691, 'n': 15745, '0': 77})\n"
     ]
    }
   ],
   "source": [
    "#For the lm approach, both train_sents and dev_sents together is to be split into hot.train and not.train, then test_sents\n",
    "#is to be split into hot_test and not_test.\n",
    "print collections.Counter([i[0] for i in train_sentis])\n",
    "print collections.Counter([i[0] for i in dev_sentis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Analysts were full of suggestions yesterday as to where Kodak should consolidate and prune its businesses , regardless of the outcome of the battle with Fuji .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('hot.train.txt', 'w')\n",
    "g = open('not.train.txt', 'w')\n",
    "#first train_sents\n",
    "for i in range(len(train_sents)):\n",
    "    if train_sentis[i][0] == 'p':\n",
    "        f.write(' '.join(train_sents[i]).encode('utf-8')+'\\n')\n",
    "    else:\n",
    "        if train_sentis[i][0] == 'n':\n",
    "            g.write(' '.join(train_sents[i]).encode('utf-8')+'\\n')\n",
    "#then dev_sents\n",
    "for i in range(len(dev_sents)):\n",
    "    if train_sentis[i][0] == 'p':\n",
    "        f.write(' '.join(dev_sents[i]).encode('utf-8')+'\\n')\n",
    "    else:\n",
    "        if train_sentis[i][0] == 'n':\n",
    "            g.write(' '.join(dev_sents[i]).encode('utf-8')+'\\n')\n",
    "f.close()\n",
    "g.close()\n",
    "#then test\n",
    "f = open('hot.test.txt', 'w')\n",
    "g = open('not.test.txt', 'w')\n",
    "#first train_sents\n",
    "for i in range(len(test_sents)):\n",
    "    if train_sentis[i][0] == 'p':\n",
    "        f.write(' '.join(test_sents[i]).encode('utf-8')+'\\n')\n",
    "    else:\n",
    "        if train_sentis[i][0] == 'n':\n",
    "            g.write(' '.join(test_sents[i]).encode('utf-8')+'\\n')\n",
    "f.close()\n",
    "g.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
